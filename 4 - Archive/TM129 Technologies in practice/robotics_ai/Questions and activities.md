
**SAQ 3.1**

- a cruise missile as described has sensors to determine path and positions, it moves and guide itself suggesting some autonomy, overall it is a mobile, autonomous, intelligent robot
- a vacuum cleaner has in a way similar properties, it guides itself, has sensor to perceive the environment and respond to it suggesting some autonomy, overall it is a mobile, autonomous, intelligent robot

**SAQ 3.2**

- a remote controller does not operate without human intervention, it has no sensor to perceive the outside word and does not move with it's own power supply, is it even a robot?
- a self kiosk does not move and does not have it's own power supply, it does not work without human interaction, however it has some sensors to perceive the product but that's it, it may be a robot but not autonomous, mobile nor intelligent

**SAQ 5.1**
- Yes, they harm people, they obey to arm people and they do not protect their existence
- It might injure a human indadvertedly

**SAQ 6.1**
- touch, smell, hearing, seeing, the last is the dominant one
- many birds see much better than humans, dogs smell much better
- a camera can be a robot sensor, a thermometer also

**SAQ 2.1**
- will go with C
- will go with A
- avoid C
- will go with B

**SAQ 3.1**
- will stop
- will go back
- about to explode

**SAQ 3.2**
- up to is false is phrase

**SAQ 3.4**
- engine hot is antecedent
- engine fail is consequent
- low oil is antecedent
- engine hot is consequent

**SAQ 3.5**
- the first two rules may suggest that if the dog is told off it is happy which contrast the third
- dogs do take a decision so I guess they have a conflict resolution strategy, listening to the owner has priority over sitting on the sofa
- they do have some level of reasoning also because the do not take always the same decision

**SAQ 4.1**
- yea

**SAQ 2.1**
- Gloria and her parents plus Robbie. Robbie is a company robot for this Child name Gloria, they play and Gloria really likes having him like a friend. People in the neighbourhood grow sceptical of robots and the mother, worried also that Gloria does not have human friends decide to get rid of Robbie. When the kid finds out she basically gets depressed, they get a dog but that does not help, they decide to move out to New York in the hope she would forget but nothing can make her forget. They decide to visit the robot factory to make her understand that Robbie was a machine, not alive, at one point Robbie is unexpectedly there, she run to him almost getting hit by a tractor in the way but Robbie being a robot is the only one fast enough to react and saves her. The parents in the end decide that the robot can stay with them
- The relation between child an robot
- In an eventual future very likely, although I hope not because a child would inevitably refuse to meet other humans with such a nice robot around

**SAQ 2.2**
- No, there is no possibility of guaranteed 100% fail-safety of any somewhat complex system
- If it existed however, yes it would probably be a good idea to add the Azimov laws to robot brains

**Activity 2.1**

Regardless of the framework my the biggest worry is the ability to implement such rules which to me seems impossible. Even ignoring that, I doubt that we will ever find a clear cut answer to these questions regarding robot behavior, especially since there is no clear answer for human behaviour either.
My suspicion is that the best course of action is a combination of the models, if the consequence of actions are known, than consequentialism makes more sense but it leaves open the question of how do you know them for certain and also it does not define what to look for in the consequences.
I like the idea of priority based rules in Asimov's laws, for example, I do believe that 'don't kill' should come before 'don't harm' and probably 'protect' should also be more important than don't harm. The best example is a person with a tumor in an arm or leg, I would say it is ethical to hurt the person by removing it (even though it is not a physical harm anymore thanks to sedatives) in order to protect them from the illness spreading and killing them. This is an example of an action that is in itself bad, after all you are cutting off someone's limb, taken knowing the consequences, as per consequentialism, which will save the person. It takes courage to make such decision, which is a virtue, and the base motive is to save the person's life, so supported by deontological ethics.
I am sure there are however many more examples that are hard to give a clear answer to and I believe that not only there is no 'one size fit all' answer, but that some situations do not have a clear answer in itself

**Activity 4.4**

- I believe every accident should be analyzed on a case-by-case basis. For example, in the case of a fully autonomous vehicle, one without even a steering wheel, the responsibility would be much more often on the shoulders of the company developing the vehicle since the passenger would have basically no say in what happens.
  However, there could be exception also to this cases, if a car operates correctly and a person deliberately throw himself in it's path, like it happens in insurance frauds, the responsibility should lie on the pedestrian just like it is under current laws.
  If however the vehicle is not fully autonomous the situation become much more complex, several questions need to be addressed before hoping to reach a conclusion, was the person supervising distracted? Did the vehicle's safety features allow for an opportunity to avoid the accident? This are just some of the crucial questions that would need to be addressed.
  
- I think it is impossible to program vehicles to behave in a truly ethical manner because ethics are not well defined even for us humans and we can recognise that because the same decision in the same situation could seem fully ethical to some while immoral to others.
  Furthermore, many ethical dilemmas have no answer at all, take the trolley problem, let's say we change it a bit. What if the trolley was about to hit a 6 years old girl and if you pull the lever it would hit a 95 years old man instead? Almost everyone would consider it ethical to pull the lever, but what if it was two 95 years old man, or 5, or 15, what if they where 90 or 85 or 80. Not only there is no answer true to everyone, and there will never be, but there is no truly correct answer at all. No one could pin point at the combination of number of people and age at which the answer would shift from pulling the lever to not pulling the lever. This is to illustrate that there is never going to be an autonomous vehicle to universally ethically, but that is not a reason to discard them. The reality is that autonomous vehicle already cause far fewer accidents and kill far fewer people than human operated once and that alone is a good reason to keep working on them regardless of if they make every decision perfectly ethically or not.

**Activity 5.1**

China low cost workers made agri move there instead of robots, did this stall progress?
How much workers work with machines on the field and how do you think this will look like in the future?

